# Ultralytics üöÄ AGPL-3.0 License - https://ultralytics.com/license

import torch
import numpy as np
import warnings
# ‚≠êÔ∏è ÂèØËßÜÂåñÂØºÂÖ•
import cv2
import os
from ultralytics.engine.predictor import BasePredictor
from ultralytics.engine.results import Results
from ultralytics.utils import ops
import warnings
import numpy as np

def prefilter_boxes(boxes, scores, labels, weights, thr):
    # Create dict with boxes stored by its label
    new_boxes = dict()

    for t in range(len(boxes)):

        if len(boxes[t]) != len(scores[t]):
            print('Error. Length of boxes arrays not equal to length of scores array: {} != {}'.format(len(boxes[t]), len(scores[t])))
            exit()

        if len(boxes[t]) != len(labels[t]):
            print('Error. Length of boxes arrays not equal to length of labels array: {} != {}'.format(len(boxes[t]), len(labels[t])))
            exit()

        for j in range(len(boxes[t])):
            score = scores[t][j]
            if score < thr:
                continue
            label = int(labels[t][j])
            box_part = boxes[t][j]
            x1 = float(box_part[0])
            y1 = float(box_part[1])
            x2 = float(box_part[2])
            y2 = float(box_part[3])

            # Box data checks
            if x2 < x1:
                warnings.warn('X2 < X1 value in box. Swap them.')
                x1, x2 = x2, x1
            if y2 < y1:
                warnings.warn('Y2 < Y1 value in box. Swap them.')
                y1, y2 = y2, y1
            if x1 < 0:
                warnings.warn('X1 < 0 in box. Set it to 0.')
                x1 = 0
            if x1 > 1:
                warnings.warn('X1 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')
                x1 = 1
            if x2 < 0:
                warnings.warn('X2 < 0 in box. Set it to 0.')
                x2 = 0
            if x2 > 1:
                warnings.warn('X2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')
                x2 = 1
            if y1 < 0:
                warnings.warn('Y1 < 0 in box. Set it to 0.')
                y1 = 0
            if y1 > 1:
                warnings.warn('Y1 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')
                y1 = 1
            if y2 < 0:
                warnings.warn('Y2 < 0 in box. Set it to 0.')
                y2 = 0
            if y2 > 1:
                warnings.warn('Y2 > 1 in box. Set it to 1. Check that you normalize boxes in [0, 1] range.')
                y2 = 1
            if (x2 - x1) * (y2 - y1) == 0.0:
                warnings.warn("Zero area box skipped: {}.".format(box_part))
                continue

            # [label, score, weight, model index, x1, y1, x2, y2]
            b = [int(label), float(score) * weights[t], weights[t], t, x1, y1, x2, y2]
            if label not in new_boxes:
                new_boxes[label] = []
            new_boxes[label].append(b)

    # Sort each list in dict by score and transform it to numpy array
    for k in new_boxes:
        current_boxes = np.array(new_boxes[k])
        new_boxes[k] = current_boxes[current_boxes[:, 1].argsort()[::-1]]

    return new_boxes


def get_weighted_box(boxes, conf_type='avg'):
    """
    Create weighted box for set of boxes
    :param boxes: set of boxes to fuse
    :param conf_type: type of confidence one of 'avg' or 'max'
    :return: weighted box (label, score, weight, model index, x1, y1, x2, y2)
    """

    box = np.zeros(8, dtype=np.float32)
    conf = 0
    conf_list = []
    w = 0
    for b in boxes:
        box[4:] += (b[1] * b[4:])
        conf += b[1]
        conf_list.append(b[1])
        w += b[2]
    box[0] = boxes[0][0]
    if conf_type in ('avg', 'box_and_model_avg', 'absent_model_aware_avg'):
        box[1] = conf / len(boxes)
    elif conf_type == 'max':
        box[1] = np.array(conf_list).max()
    box[2] = w
    box[3] = -1 # model index field is retained for consistency but is not used.
    box[4:] /= conf
    return box


def find_matching_box_fast(boxes_list, new_box, match_iou):
    """
        Reimplementation of find_matching_box with numpy instead of loops. Gives significant speed up for larger arrays
        (~100x). This was previously the bottleneck since the function is called for every entry in the array.
    """
    def bb_iou_array(boxes, new_box):
        # bb interesection over union
        xA = np.maximum(boxes[:, 0], new_box[0])
        yA = np.maximum(boxes[:, 1], new_box[1])
        xB = np.minimum(boxes[:, 2], new_box[2])
        yB = np.minimum(boxes[:, 3], new_box[3])

        interArea = np.maximum(xB - xA, 0) * np.maximum(yB - yA, 0)

        # compute the area of both the prediction and ground-truth rectangles
        boxAArea = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])
        boxBArea = (new_box[2] - new_box[0]) * (new_box[3] - new_box[1])

        iou = interArea / (boxAArea + boxBArea - interArea)

        return iou

    if boxes_list.shape[0] == 0:
        return -1, match_iou

    # boxes = np.array(boxes_list)
    boxes = boxes_list

    ious = bb_iou_array(boxes[:, 4:], new_box[4:])

    ious[boxes[:, 0] != new_box[0]] = -1

    best_idx = np.argmax(ious)
    best_iou = ious[best_idx]

    if best_iou <= match_iou:
        best_iou = match_iou
        best_idx = -1

    return best_idx, best_iou


def weighted_boxes_fusion(
        boxes_list,
        scores_list,
        labels_list,
        weights=None,
        iou_thr=0.55,
        skip_box_thr=0.0,
        conf_type='avg',
        allows_overflow=False
):
    '''
    :param boxes_list: list of boxes predictions from each model, each box is 4 numbers.
    It has 3 dimensions (models_number, model_preds, 4)
    Order of boxes: x1, y1, x2, y2. We expect float normalized coordinates [0; 1]
    :param scores_list: list of scores for each model
    :param labels_list: list of labels for each model
    :param weights: list of weights for each model. Default: None, which means weight == 1 for each model
    :param iou_thr: IoU value for boxes to be a match
    :param skip_box_thr: exclude boxes with score lower than this variable
    :param conf_type: how to calculate confidence in weighted boxes.
        'avg': average value,
        'max': maximum value,
        'box_and_model_avg': box and model wise hybrid weighted average,
        'absent_model_aware_avg': weighted average that takes into account the absent model.
    :param allows_overflow: false if we want confidence score not exceed 1.0

    :return: boxes: boxes coordinates (Order of boxes: x1, y1, x2, y2).
    :return: scores: confidence scores
    :return: labels: boxes labels
    '''

    if weights is None:
        weights = np.ones(len(boxes_list))
    if len(weights) != len(boxes_list):
        print('Warning: incorrect number of weights {}. Must be: {}. Set weights equal to 1.'.format(len(weights), len(boxes_list)))
        weights = np.ones(len(boxes_list))
    weights = np.array(weights)

    if conf_type not in ['avg', 'max', 'box_and_model_avg', 'absent_model_aware_avg']:
        print('Unknown conf_type: {}. Must be "avg", "max" or "box_and_model_avg", or "absent_model_aware_avg"'.format(conf_type))
        exit()

    filtered_boxes = prefilter_boxes(boxes_list, scores_list, labels_list, weights, skip_box_thr)
    if len(filtered_boxes) == 0:
        return np.zeros((0, 4)), np.zeros((0,)), np.zeros((0,))

    overall_boxes = []
    for label in filtered_boxes:
        boxes = filtered_boxes[label]
        new_boxes = []
        weighted_boxes = np.empty((0, 8))

        # Clusterize boxes
        for j in range(0, len(boxes)):
            index, best_iou = find_matching_box_fast(weighted_boxes, boxes[j], iou_thr)

            if index != -1:
                new_boxes[index].append(boxes[j])
                weighted_boxes[index] = get_weighted_box(new_boxes[index], conf_type)
            else:
                new_boxes.append([boxes[j].copy()])
                weighted_boxes = np.vstack((weighted_boxes, boxes[j].copy()))

        # Rescale confidence based on number of models and boxes
        for i in range(len(new_boxes)):
            clustered_boxes = new_boxes[i]
            if conf_type == 'box_and_model_avg':
                clustered_boxes = np.array(clustered_boxes)
                # weighted average for boxes
                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weighted_boxes[i, 2]
                # identify unique model index by model index column
                _, idx = np.unique(clustered_boxes[:, 3], return_index=True)
                # rescale by unique model weights
                weighted_boxes[i, 1] = weighted_boxes[i, 1] *  clustered_boxes[idx, 2].sum() / weights.sum()
            elif conf_type == 'absent_model_aware_avg':
                clustered_boxes = np.array(clustered_boxes)
                # get unique model index in the cluster
                models = np.unique(clustered_boxes[:, 3]).astype(int)
                # create a mask to get unused model weights
                mask = np.ones(len(weights), dtype=bool)
                mask[models] = False
                # absent model aware weighted average
                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / (weighted_boxes[i, 2] + weights[mask].sum())
            elif conf_type == 'max':
                weighted_boxes[i, 1] = weighted_boxes[i, 1] / weights.max()
            elif not allows_overflow:
                weighted_boxes[i, 1] = weighted_boxes[i, 1] * min(len(weights), len(clustered_boxes)) / weights.sum()
            else:
                weighted_boxes[i, 1] = weighted_boxes[i, 1] * len(clustered_boxes) / weights.sum()
        overall_boxes.append(weighted_boxes)
    overall_boxes = np.concatenate(overall_boxes, axis=0)
    overall_boxes = overall_boxes[overall_boxes[:, 1].argsort()[::-1]]
    boxes = overall_boxes[:, 4:]
    scores = overall_boxes[:, 1]
    labels = overall_boxes[:, 0]
    return boxes, scores, labels

class DetectionPredictor(BasePredictor):
    def postprocess(self, preds, img, orig_imgs, **kwargs):

        '''
        Ëøô‰∏™ËøáÁ®ãÂ§ßËá¥Â¶Ç‰∏ãÔºö
            ÁÆóÊ≥ïÈ¶ñÂÖàÂ∞ÜÊâÄÊúâÈ¢ÑÊµãÊ°ÜÊåâÁΩÆ‰ø°Â∫¶ÂàÜÊï∞‰ªéÈ´òÂà∞‰ΩéÊéíÂ∫è„ÄÇ
            ÂÆÉÂèñÂá∫ÂàÜÊï∞ÊúÄÈ´òÁöÑÈÇ£‰∏™ËìùÊ°ÜÔºå‰ª•ÂÆÉ‰∏∫Ê†∏ÂøÉÔºåÂàõÂª∫Á¨¨1‰∏™ËÅöÁ±ª„ÄÇ
            ÁÑ∂ÂêéÔºåÂÆÉÂèñÂá∫‰∏ã‰∏Ä‰∏™ËìùÊ°ÜÔºåËÆ°ÁÆóËøô‰∏™Ê°Ü‰∏éÁé∞ÊúâÊØè‰∏™ËÅöÁ±ªÁöÑ‰ª£Ë°®Ê°ÜÁöÑIoU„ÄÇ
            ÂÖ≥ÈîÆÂà§Êñ≠Ôºö
            Â¶ÇÊûúËøô‰∏™Êñ∞Ê°Ü‰∏éÁ¨¨1‰∏™ËÅöÁ±ªÁöÑ‰ª£Ë°®Ê°ÜÁöÑIoUÂ§ß‰∫éÊÇ®ËÆæÁΩÆÁöÑiou_thrÔºà‰æãÂ¶Ç0.5ÔºâÔºåÈÇ£‰πàËøô‰∏™Êñ∞Ê°ÜÂ∞±Ë¢´ËÆ§‰∏∫ÊòØ‚ÄúÁ¨¨1‰∏™ËÅöÁ±ªÁöÑÊúãÂèã‚ÄùÔºåÂπ∂Ë¢´Âä†ÂÖ•Âà∞Ëøô‰∏™ËÅöÁ±ª‰∏≠„ÄÇ
            Â¶ÇÊûúËøô‰∏™Êñ∞Ê°Ü‰∏éÊâÄÊúâÁé∞ÊúâËÅöÁ±ªÁöÑIoUÈÉΩÂ∞è‰∫éiou_thrÔºåÁÆóÊ≥ïÂ∞±‰ºöËÆ§‰∏∫ÂÆÉ‰∏çÂ±û‰∫é‰ªª‰Ωï‰∏Ä‰∏™Áé∞ÊúâÁöÑ‚ÄúÊúãÂèãÂúà‚ÄùÔºå‰∫éÊòØÂ∞±‰∏∫ÂÆÉÂàõÂª∫‰∏Ä‰∏™Êñ∞ÁöÑËÅöÁ±ªÔºà‰æãÂ¶ÇÔºåÁ¨¨2‰∏™ËÅöÁ±ªÔºâ„ÄÇ
            Ëøô‰∏™ËøáÁ®ã‰ºö‰∏ÄÁõ¥ÊåÅÁª≠ÔºåÁõ¥Âà∞ÊâÄÊúâÁöÑËìùÊ°ÜÈÉΩË¢´ÂàÜÈÖçÂà∞Êüê‰∏™ËÅöÁ±ª‰∏≠„ÄÇ
         
        '''
        #print("\n---------------use wbf--------------------------\n")

        # WBF ÂèÇÊï∞
        # Ê†πÊçÆÈáçÂè†Á®ãÂ∫¶ÔºàIoUÔºâÔºåÂ∞ÜËøô‰∫õËìùÊ°ÜÂàÜÊàê‰∏çÂêåÁöÑ‚ÄúÁæ§ÁªÑ‚ÄùÊàñ‚ÄúÁ∞á‚ÄùÔºàClusterÔºâ,Â§ß‰∫éËøô‰∏™ÂÄºÁöÑÂ∞ÜË¢´ËÅöÁ±ªÂà∞‰∏ÄËµ∑
        wbf_iou_thr = 0.01
        wbf_skip_box_thr = 0.25
        #Êï∞Âè™ÂΩ±ÂìçÊúÄÁªàÁöÑÁΩÆ‰ø°Â∫¶ÂàÜÊï∞Ôºå‰∏çÂΩ±ÂìçÂùêÊ†á„ÄÇ ÊâÄÊúâÊ°ÜÁöÑÂùêÊ†áËøõË°åÂä†ÊùÉÂπ≥ÂùáÂæóÂà∞ÁöÑÔºåÊùÉÈáçÂ∞±ÊòØÂÆÉ‰ª¨ÂêÑËá™ÁöÑÁΩÆ‰ø°Â∫¶
        conf_type = 'avg'
        
        # ‚≠êÔ∏è‰ΩøÁî® self.args Âä®ÊÄÅÊéßÂà∂ÂèØËßÜÂåñÔºåËÄå‰∏çÊòØÁ°¨ÁºñÁ†Å
        visualize = False
        self.img_shape = img.shape[2:]
        if isinstance(preds, (list, tuple)):
            preds = preds[0]

        preds = preds.permute(0, 2, 1)
        preds[..., :4] = ops.xywh2xyxy(preds[..., :4])
        # ÊØè‰∏™Ê£ÄÊµãÊ°ÜÁöÑÊúÄÈ´òÁΩÆ‰ø°Â∫¶ÂàÜÊï∞ labels: ÊØè‰∏™Ê£ÄÊµãÊ°ÜÂØπÂ∫îÁöÑÁ±ªÂà´Ê†áÁ≠æ(Á¥¢Âºï
        scores, labels = preds[..., 4:].max(-1)
        
        preds_np = preds.cpu().numpy()
        scores_np = scores.cpu().numpy()
        labels_np = labels.cpu().numpy()
        
        img_h, img_w = self.img_shape
        results_list = []

        for i in range(preds_np.shape[0]):
            pred_i, scores_i, labels_i = preds_np[i], scores_np[i], labels_np[i]

            mask = scores_i >= wbf_skip_box_thr
            boxes_i, scores_i, labels_i = pred_i[mask, :4], scores_i[mask], labels_i[mask]

            pre_wbf_boxes_pixels = boxes_i.copy() if visualize else None

            if not boxes_i.shape[0]:
                results_list.append(torch.empty((0, 6), device=preds.device))
                if visualize:  
                    self.save_visualization(orig_imgs[i], None, None, i)
                continue
            
            # ‰∏∫ WBF ÂáÜÂ§áÊï∞ÊçÆ (ÂΩí‰∏ÄÂåñ)
            boxes_i_normalized = boxes_i.copy()
            boxes_i_normalized[:, [0, 2]] /= img_w
            boxes_i_normalized[:, [1, 3]] /= img_h
            
            boxes_list_wbf = [boxes_i_normalized.tolist()]
            scores_list_wbf = [scores_i.tolist()]
            labels_list_wbf = [labels_i.tolist()]
            
            fused_boxes_normalized, fused_scores, fused_labels = weighted_boxes_fusion(
                boxes_list_wbf, scores_list_wbf, labels_list_wbf,
                weights=None, iou_thr=wbf_iou_thr, skip_box_thr=wbf_skip_box_thr, conf_type=conf_type
            )
            
            # ‚≠êÔ∏è FIX 1: Â∞ÜÂèØËßÜÂåñÈÄªËæëÁßªÂà∞Ê≠£Á°ÆÁöÑ‰ΩçÁΩÆ
            if fused_boxes_normalized.shape[0] > 0:
                # ÂèçÂΩí‰∏ÄÂåñ
                fused_boxes_pixels = fused_boxes_normalized.copy()
                fused_boxes_pixels[:, [0, 2]] *= img_w
                fused_boxes_pixels[:, [1, 3]] *= img_h
                
                # ÂêàÂπ∂ÁªìÊûú
                fused_results = torch.cat([
                    torch.from_numpy(fused_boxes_pixels),
                    torch.from_numpy(fused_scores).unsqueeze(1),
                    torch.from_numpy(fused_labels).unsqueeze(1)
                ], dim=1).to(preds.device)
                results_list.append(fused_results)

                # Âú®ÊàêÂäüËûçÂêàÂêéË∞ÉÁî®ÂèØËßÜÂåñ
                if visualize:
                    self.save_visualization(orig_imgs[i], pre_wbf_boxes_pixels, fused_boxes_pixels, i)
            else:
                results_list.append(torch.empty((0, 6), device=preds.device))
                # ‚≠êÔ∏è FIX 2: ‰øÆÊ≠£ÂèØËßÜÂåñË∞ÉÁî®ÂèÇÊï∞ÔºåÂΩìËûçÂêàÂêéÊ≤°ÊúâÊ°ÜÊó∂Ôºåpost_boxesÂ∫î‰∏∫None
                if visualize:
                    self.save_visualization(orig_imgs[i], pre_wbf_boxes_pixels, None, i)

        if not isinstance(orig_imgs, list):
            orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)

        return self.construct_results(results_list, img, orig_imgs, **kwargs)

    def save_visualization(self, orig_img, pre_boxes, post_boxes, batch_idx):
        PRE_COLOR = (255, 150, 0)  # ‰∫ÆËìùËâ≤
        POST_COLOR = (0, 255, 0)   # ÁªøËâ≤
        
        vis_img = orig_img.copy()
        img_shape = vis_img.shape
        
        if pre_boxes is not None and len(pre_boxes) > 0:
            scaled_pre_boxes = ops.scale_boxes(self.img_shape, pre_boxes, img_shape)
            for box in scaled_pre_boxes:
                x1, y1, x2, y2 = map(int, box)
                cv2.rectangle(vis_img, (x1, y1), (x2, y2), PRE_COLOR, 2) # Á∫øÊù°Âä†Á≤ó‰∏ÄÁÇπ

        if post_boxes is not None and len(post_boxes) > 0:
            scaled_post_boxes = ops.scale_boxes(self.img_shape, post_boxes, img_shape)
            for box in scaled_post_boxes:
                x1, y1, x2, y2 = map(int, box)
                cv2.rectangle(vis_img, (x1, y1), (x2, y2), POST_COLOR, 3) # Á∫øÊù°Êõ¥Á≤ó‰ª•Âå∫ÂàÜ
                cv2.putText(vis_img, 'WBF', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, POST_COLOR, 2)

        p = self.batch[0][batch_idx]
        base, ext = os.path.splitext(os.path.basename(p))
        save_dir = '/mnt/18T/liwx/H800-Mount/Code/Experiment/tools/detection_metrics/compare_wbf_2'
        os.makedirs(save_dir, exist_ok=True)
        save_path = os.path.join(save_dir, f'{base}_wbf_vis{ext}')
        
        cv2.imwrite(save_path, vis_img)
        # ‚≠êÔ∏è FIX 4: ‰ΩøÁî® LOGGER Êõø‰ª£ print
        print(f"WBF visualization saved to {save_path}")

    def get_obj_feats(self, feat_maps, idxs):
        # Ê≠§ÂáΩÊï∞Êó†ÈúÄ‰øÆÊîπ
        import torch
        s = min([x.shape[1] for x in feat_maps]); obj_feats = torch.cat([x.permute(0, 2, 3, 1).reshape(x.shape[0], -1, s, x.shape[1] // s).mean(dim=-1) for x in feat_maps], dim=1)
        return [feats[idx] if len(idx) else [] for feats, idx in zip(obj_feats, idxs)]

    def construct_results(self, preds, img, orig_imgs, **kwargs):
        # Â≠òÂÇ®Ê®°ÂûãËæìÂÖ•Â∞∫ÂØ∏‰ª•Â§áÂèØËßÜÂåñ‰ΩøÁî®
       
        self.img_shape = img.shape[2:]
        return [self.construct_result(pred, img, orig_img, img_path) for pred, orig_img, img_path in zip(preds, orig_imgs, self.batch[0])]

    def construct_result(self, pred, img, orig_img, img_path):
        # Ê≠§ÂáΩÊï∞Êó†ÈúÄ‰øÆÊîπ
        if pred.shape[0] > 0:
            pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)
        return Results(orig_img, path=img_path, names=self.model.names, boxes=pred[:, :6])
############################################################################################################
'''
class DetectionPredictor(BasePredictor):
    """
    A class extending the BasePredictor class for prediction based on a detection model.

    This predictor specializes in object detection tasks, processing model outputs into meaningful detection results
    with bounding boxes and class predictions.

    Attributes:
        args (namespace): Configuration arguments for the predictor.
        model (nn.Module): The detection model used for inference.
        batch (list): Batch of images and metadata for processing.

    Methods:
        postprocess: Process raw model predictions into detection results.
        construct_results: Build Results objects from processed predictions.
        construct_result: Create a single Result object from a prediction.
        get_obj_feats: Extract object features from the feature maps.

    Examples:
        >>> from ultralytics.utils import ASSETS
        >>> from ultralytics.models.yolo.detect import DetectionPredictor
        >>> args = dict(model="yolo11n.pt", source=ASSETS)
        >>> predictor = DetectionPredictor(overrides=args)
        >>> predictor.predict_cli()
    """

    def postprocess(self, preds, img, orig_imgs, **kwargs):
        """
        Post-process predictions and return a list of Results objects.

        This method applies non-maximum suppression to raw model predictions and prepares them for visualization and
        further analysis.

        Args:
            preds (torch.Tensor): Raw predictions from the model.
            img (torch.Tensor): Processed input image tensor in model input format.
            orig_imgs (torch.Tensor | list): Original input images before preprocessing.
            **kwargs (Any): Additional keyword arguments.

        Returns:
            (list): List of Results objects containing the post-processed predictions.

        Examples:
            >>> predictor = DetectionPredictor(overrides=dict(model="yolo11n.pt"))
            >>> results = predictor.predict("path/to/image.jpg")
            >>> processed_results = predictor.postprocess(preds, img, orig_imgs)
        """
        save_feats = getattr(self, "_feats", None) is not None
        print("\n-----------use non_max_suppression---------------------------\n")
        preds = ops.non_max_suppression(
            preds,
            self.args.conf,
            self.args.iou,
            self.args.classes,
            self.args.agnostic_nms,
            max_det=self.args.max_det,
            nc=0 if self.args.task == "detect" else len(self.model.names),
            end2end=getattr(self.model, "end2end", False),
            rotated=self.args.task == "obb",
            return_idxs=save_feats,
        )

        if not isinstance(orig_imgs, list):  # input images are a torch.Tensor, not a list
            orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)

        if save_feats:
            obj_feats = self.get_obj_feats(self._feats, preds[1])
            preds = preds[0]

        results = self.construct_results(preds, img, orig_imgs, **kwargs)

        if save_feats:
            for r, f in zip(results, obj_feats):
                r.feats = f  # add object features to results

        return results

    def get_obj_feats(self, feat_maps, idxs):
        """Extract object features from the feature maps."""
        import torch

        s = min([x.shape[1] for x in feat_maps])  # find smallest vector length
        obj_feats = torch.cat(
            [x.permute(0, 2, 3, 1).reshape(x.shape[0], -1, s, x.shape[1] // s).mean(dim=-1) for x in feat_maps], dim=1
        )  # mean reduce all vectors to same length
        return [feats[idx] if len(idx) else [] for feats, idx in zip(obj_feats, idxs)]  # for each img in batch

    def construct_results(self, preds, img, orig_imgs):
        """
        Construct a list of Results objects from model predictions.

        Args:
            preds (List[torch.Tensor]): List of predicted bounding boxes and scores for each image.
            img (torch.Tensor): Batch of preprocessed images used for inference.
            orig_imgs (List[np.ndarray]): List of original images before preprocessing.

        Returns:
            (List[Results]): List of Results objects containing detection information for each image.
        """
        return [
            self.construct_result(pred, img, orig_img, img_path)
            for pred, orig_img, img_path in zip(preds, orig_imgs, self.batch[0])
        ]

    def construct_result(self, pred, img, orig_img, img_path):
        """
        Construct a single Results object from one image prediction.

        Args:
            pred (torch.Tensor): Predicted boxes and scores with shape (N, 6) where N is the number of detections.
            img (torch.Tensor): Preprocessed image tensor used for inference.
            orig_img (np.ndarray): Original image before preprocessing.
            img_path (str): Path to the original image file.

        Returns:
            (Results): Results object containing the original image, image path, class names, and scaled bounding boxes.
        """
        pred[:, :4] = ops.scale_boxes(img.shape[2:], pred[:, :4], orig_img.shape)
        return Results(orig_img, path=img_path, names=self.model.names, boxes=pred[:, :6])
        '''
